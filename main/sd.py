import torch
import time
from utils import sample, norm_logits, max_fn, KVCacheModel
from typing import Tuple
from transformers.modeling_outputs import BaseModelOutputWithPast

class SpeculativeDecoding:
    def __init__(self, stats: bool = False):
        """
        Args:
            stats (bool, optional): whether to print stats. Defaults to True.
        """
        self.time_spend_on_draft_model_generation = 0
        self.time_spend_on_target_model_forward = 0
        self.stats = stats

    def get_time_spend_on_draft_model_generation(self):
        return self.time_spend_on_draft_model_generation

    def get_time_spend_on_target_model_forward(self):
        return self.time_spend_on_target_model_forward

    @torch.no_grad()
    def sampling_without_kvcache(self,
                                 draft_tokens: torch.Tensor,
                                 target_model: torch.nn.Module,
                                 temperature: float = 1,
                                 top_k: int = 0,
                                 top_p: float = 0) -> list:
        """
        Args:
            draft_tokens (torch.Tensor): tokens generated by draft model
            target_model (torch.nn.Module): target model for speculative decoding
            temperature (float, optional): Defaults to 1.
            top_k (int, optional): Defaults to 0.
            top_p (float, optional): Defaults to 0.
            random_seed (int, optional): Defaults to 1234.
        """
        target_model_history = target_model(draft_tokens).logits
        for i in range(target_model_history.shape[-2]):
            target_model_history[:, i, :] = norm_logits(
                target_model_history[:, i, :], temperature, top_k, top_p)
        return target_model_history

    def speculative_decoding(self,
                             input_ids: torch.Tensor,
                             draft_model: torch.nn.Module,
                             target_model: torch.nn.Module,
                             max_len: int,
                             gamma: int = 4,
                             temperature: float = 1,
                             top_k: int = 0,
                             top_p: float = 0,
                             random_seed: int = 1234) -> torch.Tensor:
        """
        Args:
            input_ids (torch.Tensor): input tensor
            draft_model (torch.nn.Module): draft model for speculative decoding
            target_model (torch.nn.Module): target model for speculative decoding
            max_len (int): maximum length of token generation 
            gamma (int, optional): gamma. Defaults to 4.
            temperature (float, optional): temperature. Defaults to 1.
            top_k (int, optional): top k. Defaults to 0.
            top_p (float, optional): top p. Defaults to 0.
            random_seed (int, optional): random seed. Defaults to 1234.
        """
        device = input_ids.device
        draft_model.to(device)
        target_model.to(device)
        
        seq_len = input_ids.shape[1]
        T = seq_len + max_len
        approx_model_cache = KVCacheModel(draft_model, temperature, top_k, top_p).to(device)

        resample_count = 0
        target_sample_count = 0
        accepted_count = 0
        input_ids = input_ids.to(device)
        start_time = time.time()
        draft_tokens = None
        total_draft_generate_count = 0

        while input_ids.shape[1] < T:
            prefix_len = input_ids.shape[1]

            draft_generate_start_time = time.time()
            draft_tokens = approx_model_cache.generate(input_ids, gamma)
            total_draft_generate_count += gamma
            draft_generate_end_time = time.time()
            self.time_spend_on_draft_model_generation += draft_generate_end_time - draft_generate_start_time

            target_forward_time = time.time()
            target_model_history_tensor = self.sampling_without_kvcache(
                draft_tokens=draft_tokens,
                target_model=target_model,
                temperature=temperature,
                top_k=top_k,
                top_p=top_p,
            )
            finish_target_forward_time = time.time()
            self.time_spend_on_target_model_forward += finish_target_forward_time - target_forward_time

            # Probability comparison (verification) on the server side
            accepted_count = 0
            resample_count = 0
            target_sample_count = 0
            n = prefix_len + gamma - 1
            for i in range(gamma):
                r = torch.rand(1, device=device)
                j = draft_tokens[:, prefix_len + i]
                if r > (target_model_history_tensor[:, prefix_len + i - 1, j]) / (approx_model_cache._prob_history[:, prefix_len + i - 1, j]):
                    n = prefix_len + i - 1
                    break
                accepted_count += 1

            input_ids = draft_tokens[:, :n + 1]

            if n < prefix_len + gamma - 1:
                t = sample(max_fn(target_model_history_tensor[:, n, :] - approx_model_cache._prob_history[:, n, :]))
                resample_count += 1
            else:
                assert n == target_model_history_tensor.shape[1] - 1
                t = sample(target_model_history_tensor[:, -1, :])
                target_sample_count += 1

            input_ids = torch.cat((input_ids, t), dim=1)
            approx_model_cache.rollback(n + 1)

        if self.stats:
            print(f"generated tokens numbers {input_ids.shape[-1] - seq_len}, accepted_count {accepted_count}, target_sample_count {target_sample_count}, resample_count {resample_count}")
        end_time = time.time()
        print(f'total time spend on speculative decoding: {end_time - start_time}')
        
        token_generate_speed = 0
        if (input_ids.shape[-1] - seq_len) >= max_len:
            token_generate_speed = max_len / (end_time - start_time)
            print(f"Token Generation Speed (with speculative decoding): {token_generate_speed} tokens/s")
        else:
            token_generate_speed = (input_ids.shape[-1] - seq_len) / (end_time - start_time)
            print(f"Token Generation Speed (with speculative decoding): {token_generate_speed} tokens/s")

        print(f"Acceptance Rate: {accepted_count / total_draft_generate_count}")
        approx_model_cache.clear_cache()
        return input_ids, accepted_count / total_draft_generate_count, token_generate_speed